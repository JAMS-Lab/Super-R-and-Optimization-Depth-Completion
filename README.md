# SODC
Abstract--- Depth completion, the process of generating dense depth maps from sparse or incomplete data, is inherently challenging due to the variability in sensor types, environmental conditions, and the differences between data modalities. In this paper, we propose a novel framework for depth completion. First, we introduce an efficient depth estimation network capable of predicting relative depth from a single RGB image. Next, we design a depth super-resolution network that refines the predicted depth by using Fast Fourier Convolution (FFC) and Gradient-weighted Symmetric Feature Transmission (GSFT) modules. These modules upsample the depth map using high-resolution RGB guidance, effectively mitigating the cross-modality gap. Finally, a global optimization step fuses the upsampled depth with sparse ground truth to produce high-quality dense depth maps. Our unified approach enhances generalization across diverse datasets while avoiding overfitting to specific depth corruption patterns. The enhancement in depth resolution and accuracy are critical for robotic applications requiring precise spatial perception, such as localization and manipulation. Experimental results on NYU-Depth V2 and SUN RGB-D benchmarks demonstrate the superiority of our approach compared to state-of-the-art approaches.
